{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gs7EDxbcZIot"
   },
   "source": [
    "\n",
    "# In the name of God the Beneficent the Merciful\n",
    "## Team: Overfit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following report is organized in this order first we go over our best reuslt and then \n",
    "\n",
    "we would mention our unsuccessful attempts\n",
    "\n",
    "## 1- Pipeline\n",
    "## 2- Preprocessing\n",
    "* Timestamp\n",
    "* Page Topic\n",
    "* one-hot encoding\n",
    "\n",
    "\n",
    "## 3- Modeling\n",
    "* Logistic Regression\n",
    "* Evaluation\n",
    "* Submit\n",
    "\n",
    "\n",
    "## 4- Unsuccessful attempts\n",
    "* Bayesian Probablity Method with joint pair wise probability\n",
    "* Label Smoothing\n",
    "* Factorization Machine\n",
    "* Wide and Deep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_oagLmw-J58G"
   },
   "source": [
    "# 1- **Pipeline**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/pipeline.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OGdsmYnIZio0"
   },
   "source": [
    "# 2- **Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "heOQi9WfZSw5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 216
    },
    "colab_type": "code",
    "id": "M8tshkJuZsOc",
    "outputId": "6540ffab-bc57-410b-8b1a-7251213274db"
   },
   "outputs": [],
   "source": [
    "!unzip drive/My\\ Drive/datadays2020_contest_public_dataset.zip "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HbIkOD3kbzB-"
   },
   "outputs": [],
   "source": [
    "df_ad = pd.read_csv('datadays2020_contest_public_dataset/ad.csv')\n",
    "df_event = pd.read_csv('datadays2020_contest_public_dataset/event.csv')\n",
    "df_page = pd.read_csv('datadays2020_contest_public_dataset/page.csv')\n",
    "df_topic = pd.read_csv('datadays2020_contest_public_dataset/page_topic.csv')\n",
    "df_train = pd.read_csv('datadays2020_contest_public_dataset/click_train.csv')\n",
    "df_test = pd.read_csv('datadays2020_contest_public_dataset/click_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BexnPXkJcq5T"
   },
   "source": [
    "## Timestamp\n",
    "\n",
    "1. We shifted the timestamp by first timestamp.\n",
    "2. Then we took modulus 24 (hours), and categorized it in  4 seqment of day.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9TFyuYLfbsoa"
   },
   "outputs": [],
   "source": [
    "t=min(df_event['timestamp'])\n",
    "timetable = [(0,6), (6,12),(12,18),(18,24)]\n",
    "def discretiseTime(t,timetable):\n",
    "    t=t%24\n",
    "    for i , e in enumerate(timetable):\n",
    "        #print(t , e)\n",
    "        if t>=e[0] and t<= e[1]:\n",
    "            return i\n",
    "\n",
    "df['timestamp'] = df['timestamp'].apply(lambda x : discretiseTime((x-t)/(1000*3600) , timetable)) \n",
    "dft['timestamp'] = dft['timestamp'].apply(lambda x : discretiseTime((x-t)/(1000*3600) , timetable))                                                 \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SUQCrs6deWEy"
   },
   "source": [
    "**Page Topic**\n",
    "\n",
    "first we have understood that number of pagetopics with acceptable confidence is large so we discard the confidence entirely "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ia0DKTaffNIr"
   },
   "outputs": [],
   "source": [
    "x = len(df_pageTopic['pageId'].unique())\n",
    "a = len(df_pageTopic.loc[df_pageTopic['confidence'] > 0.5]) # with bigger than 50 percent probablity\n",
    "b = len(df_pageTopic.loc[df_pageTopic['confidence'] > 0.6]) # with bigger than 60 percent probablity\n",
    "c = len(df_pageTopic.loc[df_pageTopic['confidence'] > 0.8]) # with bigger than 80 percent probablity\n",
    "\n",
    "print(\"We have estimation for {} pageId\".format(c))\n",
    "print(\"For 0.5 confidence, the ratio is {}\".format(a/x))\n",
    "print(\"For 0.6 confidence, the ratio is {}\".format(b/x))\n",
    "print(\"For 0.8 confidence, the ratio is {}\".format(c/x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FhL2JCFgxpr4"
   },
   "source": [
    "So we just kept topicId with maximum confidence value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cfl17RAfeNTP"
   },
   "outputs": [],
   "source": [
    "df_topic=df_topic.sort_values(['pageId' , 'confidence'] , ascending=False)\n",
    "df_topic=df_topic.drop_duplicates(['pageId'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3M50Agt_x_pq"
   },
   "source": [
    "**Merge**\n",
    "\n",
    "**df** := result of merging all dataframe except words and image with train data\n",
    "\n",
    "**dft** := result of merging all dataframe except words and image with test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zx-svNLQyHwm"
   },
   "outputs": [],
   "source": [
    "df = pd.merge(df_train , df_event , on='displayId' , how='left')\n",
    "dft = pd.merge(df_test , df_event , on='displayId' , how='left')\n",
    "df = pd.merge(df , df_page , on='pageId' , how='left')\n",
    "dft = pd.merge(dft , df_page , on='pageId' , how='left')\n",
    "df_topic=df_topic.sort_values(['pageId' , 'confidence'] , ascending=False)\n",
    "df_topic=df_topic.drop_duplicates(['pageId'])\n",
    "df = pd.merge(df , df_topic , on='pageId' , how='left')\n",
    "dft = pd.merge(dft , df_topic , on='pageId' , how='left')\n",
    "df = pd.merge(df , df_ad , on='adId' , how='left')\n",
    "dft = pd.merge(dft , df_ad , on='adId' , how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DvsigTaNy0mw"
   },
   "source": [
    "**Fill null value**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2yrLJKTxzAVe"
   },
   "outputs": [],
   "source": [
    "df['topicId'] = df['topicId'].fillna(0)\n",
    "df['confidence'] = df['confidence'].fillna(0)\n",
    "dft['topicId'] = dft['topicId'].fillna(0)\n",
    "dft['confidence'] = dft['confidence'].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UO6bENX7zJF4"
   },
   "source": [
    "**Changing type for better speed and RAM usage** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9I9kNEJXzGw1"
   },
   "outputs": [],
   "source": [
    "df['clicked'] = df['clicked'].astype(np.int8)\n",
    "df['timestamp'] = df['timestamp'].astype(np.int8)\n",
    "df['pageId'] = df['pageId'].astype(np.int32)\n",
    "df['widgetId'] = df['widgetId'].astype(np.int16)\n",
    "df['userId'] = df['userId'].astype(np.int32)\n",
    "df['device'] = df['device'].astype(np.int8)\n",
    "df['OS'] = df['OS'].astype(np.int8)\n",
    "df['browser'] = df['browser'].astype(np.int8)\n",
    "df['website'] = df['website'].astype(np.int32)\n",
    "df['publisher'] = df['publisher'].astype(np.int16)\n",
    "df['topicId']= df['topicId'].astype(np.int8)\n",
    "df['confidence'] =df['confidence'].astype(np.float32)\n",
    "df['campaignId'] = df['campaignId'].astype(np.int16)\n",
    "df['advertiserId'] = df['advertiserId'].astype(np.int16)\n",
    "\n",
    "#dft['clicked'] = dft['clicked'].astype(np.int8)\n",
    "dft['timestamp'] = dft['timestamp'].astype(np.int8)\n",
    "dft['pageId'] = dft['pageId'].astype(np.int32)\n",
    "dft['widgetId'] = dft['widgetId'].astype(np.int16)\n",
    "dft['userId'] = dft['userId'].astype(np.int32)\n",
    "dft['device'] = dft['device'].astype(np.int8)\n",
    "dft['OS'] = dft['OS'].astype(np.int8)\n",
    "dft['browser'] = dft['browser'].astype(np.int8)\n",
    "dft['website'] = dft['website'].astype(np.int32)\n",
    "dft['publisher'] = dft['publisher'].astype(np.int16)\n",
    "dft['topicId']= dft['topicId'].astype(np.int8)\n",
    "dft['confidence'] =dft['confidence'].astype(np.float32)\n",
    "dft['campaignId'] = dft['campaignId'].astype(np.int16)\n",
    "dft['advertiserId'] = dft['advertiserId'].astype(np.int16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1kdDrEi22-BO"
   },
   "source": [
    "Features (12 columns) for first logistic regression \n",
    "\n",
    "(intiution was it would capture a general notion of importance on all features and can be used as base estimate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gw4YssSw1ig5"
   },
   "outputs": [],
   "source": [
    "features = ['userId','adId' , 'timestamp' , 'device' , 'browser' , 'OS' , 'topicId'   , 'campaignId' , 'advertiserId' , 'widgetId' , 'website' , 'publisher']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k_rwUFc44wBI"
   },
   "source": [
    "Features1 is less inclusive to distinguish between different ads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9MAQMKhJ245N"
   },
   "outputs": [],
   "source": [
    "features1 = ['adId'  , 'topicId'   , 'campaignId' , 'advertiserId' , 'widgetId']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This heat map shows relative importance of different features pair-wise \n",
    "\n",
    "which persuade us choose above features as our distinguishing factors between ads "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/heatmap.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oRWxDV-o29Z0"
   },
   "outputs": [],
   "source": [
    "dftemp = df[features]\n",
    "dfttemp = dft[features]\n",
    "dftemp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9hu300of5Y-M"
   },
   "outputs": [],
   "source": [
    "dftemp1 = df[features1]\n",
    "dfttemp1 = dft[features1]\n",
    "dftemp1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NlGTwIzb5qv2"
   },
   "source": [
    "**Spliting test and train data**\n",
    "\n",
    "We define double test and train because of using ensemble method in our model that bring after in modelling part "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QTrhOzb0GHDG"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RSYmWM5q5eDr"
   },
   "outputs": [],
   "source": [
    "X_train , X_test , Y_train , Y_test = train_test_split(dftemp , df['clicked'] , test_size = 0.33 , random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SiYoJioj5hPG"
   },
   "outputs": [],
   "source": [
    "X_train1 , X_test1 , Y_train1 , Y_test1 = train_test_split(dftemp1 , df['clicked'] , test_size = 0.33 , random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spliting data in equal portion for those algorithms without a balancing feature built in \n",
    "\n",
    "because of class imbalance problem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#it should only be run for algorithms without built in balacing\n",
    "n = int(0.66*len(df))\n",
    "tplus = df[df['clicked']>0].sample(n=n//2 , replace=True)\n",
    "tminus = df[df['clicked']==0].sample(n=n//2 , replace=True)\n",
    "X_train = pd.concat([tplus ,tminus])\n",
    "X_train = X_train.sample(frac=1.00)\n",
    "Y_train = X_train['clicked']\n",
    "X_train = X_train[features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XnQi9pQJakFo"
   },
   "source": [
    "## **One-Hot Encoding**\n",
    "\n",
    "because of categorical data, we need to use one-hot encoder from sklearn library. and keep then **csr matrix**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V7rDq2J35qKa"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer \n",
    "transformer = ColumnTransformer(  \n",
    "    transformers=[\n",
    "        (\"all\",        # Just a name\n",
    "         OneHotEncoder(dtype = np.int8), # The transformer class\n",
    "         [0,1,2,3,4,5,6,7,8,9,10,11]           # The column(s) to be applied on.\n",
    "         ),\n",
    "    ], remainder='passthrough'\n",
    ")\n",
    "\n",
    "transformer.fit(dftemp.append(dfttemp)) # in order to capture all categorizes we merge both train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BNkcNUqx6t3K"
   },
   "outputs": [],
   "source": [
    "data = transformer.transform(X_train)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EtaQy4Tu-mf1"
   },
   "outputs": [],
   "source": [
    "data = transformer.transform(X_train1)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dRi41yu465tK"
   },
   "source": [
    "# 3- **Modeling**\n",
    "\n",
    "Our model is based on Logistic Regression method for classification of our data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Cs9cOsPt64bf"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JbYohJgb7svr"
   },
   "source": [
    "## Logistic Regression\n",
    "**The Logistic Regression parameter** \n",
    "\n",
    "C is the reverse of lambda value in cost function. After exhusive seaech on **(0.6,1)** interval we found **0.72** as optimal value\n",
    "\n",
    "Class_weight is **balanced** because of the fact that 77 percent of the value is 0 and would push the algorithm to produce zeros and would weight columns with more zeros in them which is not desirable to encounter this;we balance the classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PaI4YE4q7sDZ"
   },
   "outputs": [],
   "source": [
    "clf = LogisticRegression(class_weight='balanced' , C=0.72).fit(data,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EIMl9xnK8pDS"
   },
   "outputs": [],
   "source": [
    "clf5 = LogisticRegression(class_weight='balanced' , C=0.72).fit(data,Y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WzHxZiKq-Lqz"
   },
   "outputs": [],
   "source": [
    "Xtest = dft\n",
    "datates = transformer.transform(dfttemp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9tbV1EfO-QSB"
   },
   "outputs": [],
   "source": [
    "Xtest1 = dft1 \n",
    "datates1 = transformer.transform(dfttemp1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nL6jvZoRD1gT"
   },
   "source": [
    "## **Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0gZvpJE2GAk0"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tNBPblMaFW1S"
   },
   "outputs": [],
   "source": [
    "clf.score(data,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2LvnZEo5GSk_"
   },
   "outputs": [],
   "source": [
    "clf5.score(data,Y_train1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "byE9c-fu_C-m"
   },
   "source": [
    "## **Creating submit dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OMppWSv_--1y"
   },
   "outputs": [],
   "source": [
    "Xtest['rank'] = clf.predict_proba(datates)[:,1] + clf5.predict_proba(datates1)[:,1] #ensembling two models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4Yq71P8T-0X9"
   },
   "outputs": [],
   "source": [
    "diad={}\n",
    "for i,r in Xtest.iterrows():\n",
    "    if r['displayId'] in diad:\n",
    "        diad[r['displayId']].append(i)\n",
    "    else:\n",
    "        diad[r['displayId']] = [i,]\n",
    "for i in diad:\n",
    "    diad[i] = sorted(diad[i] , key =lambda x: -1 * Xtest.iloc[x]['rank'] )\n",
    "arr = np.zeros(len(Xtest))\n",
    "\n",
    "for i,r in Xtest.iterrows():\n",
    "    arr[i] = diad[r['displayId']].index(i) + 1\n",
    "Xtest['ranked'] = arr\n",
    "#dftest.to_csv('../submit/sub4.csv',header=None,index=False)\n",
    "dfsub = Xtest[['displayId' , 'adId' ,'ranked']]\n",
    "dfsub['ranked'] = Xtest['ranked'].apply(lambda x : int(x) )\n",
    "dfsub.head()\n",
    "dfsub.to_csv('Sub29.csv',header=None,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Aq-TfgQm_oZD"
   },
   "source": [
    "# 4- **Unsuccessful attempts**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Oakb7x1s_9AQ"
   },
   "source": [
    "## Bayesian Probablity Method with joint pair wise probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchors = ['adId' , 'campaignId' , 'advertiserId']\n",
    "feature = ['pageId','userId','widgetId' , 'topicId' ,'website' , 'timestamp']\n",
    "features = [(anchor , f) for anchor in anchors for f in feature]\n",
    "column_names = anchors + features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G9XrQJEU_8kI"
   },
   "outputs": [],
   "source": [
    "def create_real_train4(anchors , features,column_names,x_train ):\n",
    "    count=0;\n",
    "    n = len(x_train)\n",
    "    print(n)\n",
    "    dfdummy = pd.DataFrame(columns = column_names)\n",
    "    featuregroups = {}\n",
    "    for anchor in anchors:\n",
    "        featuregroups[anchor] = dftrain.groupby(anchor).mean()['clicked']\n",
    "    for f in features:\n",
    "        featuregroups[f] = dftrain.groupby(f).mean()['clicked']\n",
    "    temps=[]\n",
    "    for i,r in x_train.iterrows():\n",
    "        count+=1\n",
    "        if(count%100000==0):\n",
    "            print(\"--{}--\".format(count/n))\n",
    "        temp=[]\n",
    "        for anchor in anchors:\n",
    "            tr = featuregroups[anchor].get(r[anchor])\n",
    "            temp.append(tr if tr!=None else 0)\n",
    "        for f in features:\n",
    "            tr = featuregroups[f].get((r[f[0]] , r[f[1]]))\n",
    "            temp.append(tr if tr!=None else 0)\n",
    "        temps.append(temp)\n",
    "    dfdummy=dfdummy.append(pd.DataFrame(temps, columns=column_names) , ignore_index=True)\n",
    "    return dfdummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PeJUfZ2kDnZw"
   },
   "outputs": [],
   "source": [
    "X = create_real_train(features[:] , column_names , X_train, word_prob=probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SuzMl6stDunl"
   },
   "outputs": [],
   "source": [
    "\n",
    "clf = RandomForestClassifier(class_weight = 'balanced').fit(X , y_train['clicked'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Smoothing\n",
    "\n",
    "using label smoothing to change classification to regression which showed enhancing results in competitions like image net\n",
    "\n",
    "[Rethinking the Inception Architecture for Computer Vision](https://arxiv.org/abs/1512.00567)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ul=[]\n",
    "def add_to(x):\n",
    "    ul.append(x)\n",
    "    return 1\n",
    "temp=dftrainY['displayId'].value_counts()\n",
    "temp.index=temp.index.astype(str)\n",
    "td = temp.to_dict()\n",
    "dftrainY['displayId'] = dftrainY['displayId'].astype(str)\n",
    "\n",
    "#df['displayId'] = df['displayId'].astype(int)\n",
    "\n",
    "dftrainY['Frecuency'] = dftrainY['displayId'].apply(lambda x : td.get(x) if td.get(x)!=None else add_to(x))\n",
    "\n",
    "\n",
    "dftrainY[\"probability\"] = (dftrainY[\"clicked\"]+1)/(dftrainY[\"Frecuency\"]*2)\n",
    "\n",
    "#print(df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Etl6pprqAbYF"
   },
   "source": [
    "## Factorization Machine\n",
    "\n",
    "Factorization Machines\n",
    "(FM) which are a new model class that combines the advantages\n",
    "of Support Vector Machines (SVM) with factorization models\n",
    "thus\n",
    "they are able to estimate interactions even in problems with huge\n",
    "sparsity (like recommender systems) where SVMs fail.\n",
    "\n",
    "[Factorization Machines Steffen Rendle 2010](https://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf)\n",
    "\n",
    "[Field-aware Factorization Machines for CTR Prediction](https://www.csie.ntu.edu.tw/~cjlin/papers/ffm.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BOFeLPo2AjqI"
   },
   "outputs": [],
   "source": [
    "class FactorizationMachine(nn.Module):\n",
    "    def __init__(self, nb_features, dim_embed=50, isClassifier=True, withCuda=True):\n",
    "        super(FactorizationMachine, self).__init__()\n",
    "        \n",
    "        self.nb_features = nb_features\n",
    "        self.dim_embed = dim_embed\n",
    "        self.isClassifier = isClassifier # binary-classifier or regression\n",
    "        \n",
    "        # Stores the bias term\n",
    "        if withCuda: # WARNING : not working at present with CUDA due to type mismatch\n",
    "            self.B = Variable(torch.randn((1)).type(torch.cuda.FloatTensor), requires_grad=True)\n",
    "        else:\n",
    "            self.B = Variable(torch.randn((1)).type(torch.FloatTensor), requires_grad=True)\n",
    "        \n",
    "        # Stores the weights for the linear terms\n",
    "        self.embeddingL = nn.Embedding(nb_features, 1, padding_idx=0, max_norm=None, norm_type=2)\n",
    "        \n",
    "        # Stores the weights for the quadratic FM terms\n",
    "        self.embeddingQ = nn.Embedding(nb_features, dim_embed, padding_idx=0, max_norm=None, norm_type=2)\n",
    "\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \n",
    "        # The linear part\n",
    "        eL = self.embeddingL(X)\n",
    "        logitL = eL.sum(dim=1)\n",
    "        \n",
    "        # The Quadratic-FM part using the O(kn) formulation from Steffen Rendle\n",
    "        eQ = self.embeddingQ(X)\n",
    "        logitFM1 = eQ.mul(eQ).sum(1).sum(2)\n",
    "        z = eQ.sum(dim=1)# sum across features\n",
    "        z2 = z.mul(z) # element-wise product\n",
    "        logitFM2 = z2.sum(dim=2) # sum across embedding dimensions\n",
    "        logitFM = (logitFM1 - logitFM2)*0.5\n",
    "        \n",
    "        # Total logit\n",
    "        logit = (logitL + logitFM).squeeze(dim=-1).squeeze(dim=-1)\n",
    "        logit+= self.B.expand(1, logit.size()[0]).transpose(0,1)\n",
    "        \n",
    "        if self.isClassifier:\n",
    "            return F.sigmoid(logit)\n",
    "        else:\n",
    "            return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lMdxTOhHB50z"
   },
   "outputs": [],
   "source": [
    "model = FactorizationMachine(100, dim_embed=50, isClassifier=True, withCuda=use_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HX7BBYEQBwAo"
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "momentum = 0.9\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "\n",
    "#loss_function = nn.NLLLoss()\n",
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or using [tffm](https://github.com/geffy/tffm)\n",
    "package an open source package implementing factorization Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tffm import TFFMClassifier \n",
    "order = 3\n",
    "model = TFFMClassifier(\n",
    "    order=order, \n",
    "    rank=8, \n",
    "    optimizer=tf.train.AdamOptimizer(learning_rate=0.001), \n",
    "    n_epochs=50, \n",
    "    batch_size=512,\n",
    "    init_std=0.001,\n",
    "    reg=0.001,\n",
    "    input_type='sparse',\n",
    "    seed=42\n",
    ")\n",
    "model.fit(data, Y_train.values, show_progress=True)\n",
    "predictions = model.predict(data)\n",
    "print('[order={}] accuracy: {}'.format(order, accuracy_score(Y_train.values, predictions)))\n",
    "predictions = model.predict(datatest)\n",
    "print('[order={}] accuracy: {}'.format(order, accuracy_score(Y_test.values, predictions)))\n",
    "#model.destroy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lUFvFMp-Aj6v"
   },
   "source": [
    "## Wide and Deep \n",
    "\n",
    "Wide & Deep learning—jointly trained wide linear models\n",
    "and deep neural networks—to combine the benefits of memorization and generalization for recommender systems\n",
    "\n",
    "[Wide and Deep Learning for Recommender Systems 2016](https://arxiv.org/abs/1606.07792)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9JX9nWc9Apfe"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from keras.models import Sequential , Model\n",
    "from keras.layers import Input , Dense , concatenate\n",
    "\n",
    "#wide\n",
    "wide = Input(shape=(data.shape[1],))\n",
    "\n",
    "#deep\n",
    "deep_data = Input(shape=(data.shape[1],))\n",
    "deep = Dense(input_dim=data.shape[1] , output_dim=256 , activation='relu')(deep_data)\n",
    "deep = Dense(128 , activation = 'relu')(deep)\n",
    "\n",
    "#wide & deep\n",
    "wide_deep = concatenate([wide , deep])\n",
    "wide_deep = Dense(1 , activation='sigmoid')(wide_deep)\n",
    "model = Model(inputs=[wide , deep_data] , outputs=wide_deep)\n",
    "\n",
    "model.compile(\n",
    "    optimizer = 'rmsprop' ,\n",
    "    loss = 'binary_crossentropy' ,\n",
    "    metrics = ['accuracy']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vh9xhnaRFsaT"
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('../my_model.h5') #if there were any prior training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uS3ErQaxFr7B"
   },
   "outputs": [],
   "source": [
    "model.fit([data , data] , Y_train , epochs=5 , batch_size=512)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "XnQi9pQJakFo",
    "Aq-TfgQm_oZD",
    "Oakb7x1s_9AQ"
   ],
   "name": "Report-Overfit.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
